{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0dd0df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 'outer.ply' with 10840 points.\n",
      "\n",
      "Visualizing the point cloud with its Oriented Bounding Box (Green).\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Load the Point Cloud ---\n",
    "# The filename has been updated to 'outer.ply'\n",
    "try:\n",
    "    pcd = o3d.io.read_point_cloud(\"outer.ply\")\n",
    "    if not pcd.has_points():\n",
    "        raise ValueError(\"The loaded PLY file contains no points.\")\n",
    "    print(f\"Successfully loaded 'outer.ply' with {len(pcd.points)} points.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading point cloud: {e}\")\n",
    "    print(\"Please ensure 'outer.ply' exists in the same directory as your Jupyter notebook.\")\n",
    "    # Create a dummy point cloud for demonstration if loading fails\n",
    "    print(\"Generating a dummy point cloud for demonstration purposes.\")\n",
    "    np.random.seed(42)\n",
    "    points = np.random.rand(1000, 3) * 10\n",
    "    # Make it a bit elongated and rotated to show OBB advantage\n",
    "    points[:, 0] = points[:, 0] * 2 + points[:, 1] * 0.5\n",
    "    points[:, 1] = points[:, 1] * 1.5\n",
    "    # Apply a rotation\n",
    "    rotation_matrix = o3d.geometry.get_rotation_matrix_from_xyz((np.pi/4, np.pi/6, np.pi/8))\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(np.dot(points, rotation_matrix.T))\n",
    "\n",
    "\n",
    "# --- 2. Generate the Oriented Bounding Box (OBB) ---\n",
    "# This computes the tightest-fitting box around the points.\n",
    "obb = pcd.get_oriented_bounding_box()\n",
    "\n",
    "\n",
    "# --- 3. Visualize the Point Cloud and OBB ---\n",
    "\n",
    "# Set OBB color to green for visibility\n",
    "obb.color = (0, 1, 0) \n",
    "\n",
    "print(\"\\nVisualizing the point cloud with its Oriented Bounding Box (Green).\")\n",
    "# This opens an interactive 3D window\n",
    "o3d.visualization.draw_geometries([pcd, obb])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6aa9f72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identifying the correct side plane for visualization...\n",
      "Displaying Point Cloud, OBB (Green), and the corrected Highlighted Plane (Blue).\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Locate and Highlight a Specific OBB Plane ---\n",
    "\n",
    "print(\"\\nIdentifying the correct side plane for visualization...\")\n",
    "\n",
    "# Get the geometric properties from the calculated OBB\n",
    "obb_center = obb.center\n",
    "obb_rotation = obb.R\n",
    "obb_extent = obb.extent\n",
    "\n",
    "# To find the correct face, we sort the OBB's dimensions (extents)\n",
    "# sorted_indices[0] = index of the smallest dimension (thickness)\n",
    "# sorted_indices[1] = index of the middle dimension (width)\n",
    "# sorted_indices[2] = index of the largest dimension (length)\n",
    "sorted_indices = np.argsort(obb_extent)\n",
    "\n",
    "# --- KEY CHANGE IS HERE ---\n",
    "# Instead of the middle axis (width), we now choose the smallest axis (thickness)\n",
    "# to get the other perpendicular face.\n",
    "plane_axis_idx = sorted_indices[0] # Changed from sorted_indices[1]\n",
    "\n",
    "# The normal vector of the plane is the axis vector from the rotation matrix\n",
    "normal_vector = obb_rotation[:, plane_axis_idx]\n",
    "\n",
    "# The extent (thickness) of the box along this axis\n",
    "plane_thickness = obb_extent[plane_axis_idx]\n",
    "\n",
    "# Calculate the center point of the specific face we want to highlight.\n",
    "# We start at the OBB's center and move half the thickness along the normal vector.\n",
    "face_center = obb_center - normal_vector * (plane_thickness / 2.0)\n",
    "\n",
    "\n",
    "# --- 5. Create a Visual Mesh for the Plane ---\n",
    "\n",
    "# To visualize the plane, we create a very thin but large box mesh.\n",
    "# We create a copy of the extents to modify for our visual mesh.\n",
    "plane_mesh_extent = np.copy(obb_extent)\n",
    "\n",
    "# First, we make the mesh flat by squashing the dimension along our normal vector.\n",
    "plane_mesh_extent[plane_axis_idx] = 0.01\n",
    "\n",
    "# Next, we enlarge the other two dimensions (width and length) to make the plane visible.\n",
    "other_indices = [i for i in range(3) if i != plane_axis_idx]\n",
    "plane_mesh_extent[other_indices] *= 1.2\n",
    "\n",
    "# Create a new OBB that represents our visual plane\n",
    "plane_obb = o3d.geometry.OrientedBoundingBox(face_center, obb_rotation, plane_mesh_extent)\n",
    "\n",
    "# Create the actual mesh from this plane-like OBB\n",
    "plane_mesh = o3d.geometry.TriangleMesh.create_from_oriented_bounding_box(plane_obb)\n",
    "\n",
    "# Give the plane a distinct color (e.g., blue)\n",
    "plane_mesh.paint_uniform_color([0.1, 0.2, 0.8])\n",
    "\n",
    "# --- 6. Final Visualization ---\n",
    "\n",
    "print(\"Displaying Point Cloud, OBB (Green), and the corrected Highlighted Plane (Blue).\")\n",
    "o3d.visualization.draw_geometries([pcd, obb, plane_mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d91226ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mirroring the point cloud across the highlighted plane...\n",
      "Displaying Original PCD, OBB (Green), Plane (Blue), and Mirrored PCD (Magenta).\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Mirror the Point Cloud Using the Selected Plane ---\n",
    "\n",
    "print(\"\\nMirroring the point cloud across the highlighted plane...\")\n",
    "\n",
    "# The two key components of our mirror plane are already defined:\n",
    "# 'face_center' (a point on the plane, P₀)\n",
    "# 'normal_vector' (the normal of the plane, n)\n",
    "\n",
    "# We get the original points as a NumPy array\n",
    "points = np.asarray(pcd.points)\n",
    "\n",
    "# This is the vectorized formula for reflection: P' = P - 2 * dot(P - P₀, n) * n\n",
    "# 1. Calculate vectors from each point to the plane\n",
    "vecs_to_plane = points - face_center\n",
    "# 2. Calculate the dot product (projection) of these vectors onto the normal\n",
    "# This gives the distance of each point to the plane along the normal\n",
    "dists = np.dot(vecs_to_plane, normal_vector)\n",
    "# 3. Apply the formula to get the new mirrored points\n",
    "# We scale the normal vector by twice the distance and subtract it from the original points\n",
    "mirrored_points = points - 2 * np.outer(dists, normal_vector)\n",
    "\n",
    "# --- 8. Create and Visualize the Mirrored Point Cloud ---\n",
    "\n",
    "# Create a new Open3D point cloud object for the mirrored data\n",
    "mirrored_pcd = o3d.geometry.PointCloud()\n",
    "mirrored_pcd.points = o3d.utility.Vector3dVector(mirrored_points)\n",
    "\n",
    "# Give the new mirrored cloud a distinct color (e.g., magenta)\n",
    "mirrored_pcd.paint_uniform_color([1, 0, 1])\n",
    "\n",
    "# Final visualization including the original cloud, OBB, plane, and the new mirrored cloud\n",
    "print(\"Displaying Original PCD, OBB (Green), Plane (Blue), and Mirrored PCD (Magenta).\")\n",
    "o3d.visualization.draw_geometries([pcd, obb, plane_mesh, mirrored_pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f0bdb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 'middle.ply' with 29014 points.\n",
      "Displaying original 'outer.ply' and reference 'middle.ply' (Yellow).\n"
     ]
    }
   ],
   "source": [
    "# --- 9. Load the Reference Point Cloud ---\n",
    "\n",
    "try:\n",
    "    # Load the new reference point cloud\n",
    "    ref_pcd = o3d.io.read_point_cloud(\"middle.ply\")\n",
    "    if not ref_pcd.has_points():\n",
    "        raise ValueError(\"The loaded PLY file 'middle.ply' contains no points.\")\n",
    "    print(f\"Successfully loaded 'middle.ply' with {len(ref_pcd.points)} points.\")\n",
    "\n",
    "    # Give the reference cloud a distinct color (e.g., yellow)\n",
    "    ref_pcd.paint_uniform_color([1, 1, 0])\n",
    "\n",
    "    # Visualize the original 'outer.ply' (pcd) and the new 'middle.ply' (ref_pcd) together\n",
    "    print(\"Displaying original 'outer.ply' and reference 'middle.ply' (Yellow).\")\n",
    "    o3d.visualization.draw_geometries([pcd, ref_pcd])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please ensure 'middle.ply' is in the same directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "347667f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Before -> Mirrored cloud points: 10840\n",
      "Before -> Reference cloud points: 29014\n",
      "\n",
      "Step 1: Denoising clouds...\n",
      "Step 2: Downsampling clouds with a voxel size of 0.01...\n",
      "\n",
      "Preprocessing complete.\n",
      "After -> Mirrored cloud points: 2190\n",
      "After -> Reference cloud points: 3613\n",
      "\n",
      "Displaying the processed mirrored (Magenta) and reference (Yellow) clouds.\n"
     ]
    }
   ],
   "source": [
    "# --- 10. Denoise and Downsample Both Point Clouds ---\n",
    "\n",
    "print(\"Starting preprocessing...\")\n",
    "print(f\"Before -> Mirrored cloud points: {len(mirrored_pcd.points)}\")\n",
    "print(f\"Before -> Reference cloud points: {len(ref_pcd.points)}\")\n",
    "\n",
    "# --- Step 1: Denoising using Statistical Outlier Removal ---\n",
    "# This removes sparse outlier points.\n",
    "# nb_neighbors: How many neighbors to consider for the average distance calculation.\n",
    "# std_ratio: The standard deviation multiplier. A smaller value makes the filter more aggressive.\n",
    "print(\"\\nStep 1: Denoising clouds...\")\n",
    "mirrored_denoised, _ = mirrored_pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "ref_denoised, _ = ref_pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "\n",
    "\n",
    "# --- Step 2: Downsampling using Voxel Grid ---\n",
    "# This reduces the number of points while preserving the shape by averaging points within each 3D box (voxel).\n",
    "#\n",
    "# !!! IMPORTANT !!!\n",
    "# You MUST tune the 'voxel_size'. It depends on the scale of your object.\n",
    "# - A LARGER voxel_size (e.g., 0.05) will downsample more aggressively (fewer points).\n",
    "# - A SMALLER voxel_size (e.g., 0.005) will result in a denser cloud (more points).\n",
    "# Start with a value like 0.01 and adjust it based on your results.\n",
    "voxel_size = 0.01\n",
    "print(f\"Step 2: Downsampling clouds with a voxel size of {voxel_size}...\")\n",
    "mirrored_processed = mirrored_denoised.voxel_down_sample(voxel_size)\n",
    "ref_processed = ref_denoised.voxel_down_sample(voxel_size)\n",
    "\n",
    "print(\"\\nPreprocessing complete.\")\n",
    "print(f\"After -> Mirrored cloud points: {len(mirrored_processed.points)}\")\n",
    "print(f\"After -> Reference cloud points: {len(ref_processed.points)}\")\n",
    "\n",
    "\n",
    "# --- Final Visualization of Processed Clouds ---\n",
    "print(\"\\nDisplaying the processed mirrored (Magenta) and reference (Yellow) clouds.\")\n",
    "o3d.visualization.draw_geometries([mirrored_processed, ref_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58de0bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-orienting normals for accurate registration...\n",
      "Normals have been re-oriented.\n"
     ]
    }
   ],
   "source": [
    "# --- 10.5: Unify Normals for Correct ICP Alignment ---\n",
    "\n",
    "# IMPORTANT: Run this cell AFTER preprocessing and BEFORE registration.\n",
    "# Define source and target from your preprocessed clouds.\n",
    "source_to_fix = mirrored_processed\n",
    "target_to_fix = ref_processed\n",
    "\n",
    "print(\"Re-orienting normals for accurate registration...\")\n",
    "\n",
    "# 1. Estimate normals for both clouds\n",
    "# (Even if done later in ICP, we do it here to orient them)\n",
    "source_to_fix.estimate_normals()\n",
    "target_to_fix.estimate_normals()\n",
    "\n",
    "# 2. Orient the normals consistently\n",
    "# This function makes all normals point towards a virtual camera at [0,0,0].\n",
    "# This is a common method to make normal directions consistent.\n",
    "source_to_fix.orient_normals_towards_camera_location([0, 0, 0])\n",
    "target_to_fix.orient_normals_towards_camera_location([0, 0, 0])\n",
    "\n",
    "print(\"Normals have been re-oriented.\")\n",
    "\n",
    "# Now, you can proceed to the registration cell (Cell 11).\n",
    "# The variables 'mirrored_processed' and 'ref_processed' have been updated with correct normals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9533f6",
   "metadata": {},
   "source": [
    "# --- 11. Registration using Custom Fingerprinting ---\n",
    "import time\n",
    "from itertools import combinations\n",
    "\n",
    "# --- Step 0: Define Source and Target for clarity ---\n",
    "source = mirrored_processed\n",
    "target = ref_processed\n",
    "\n",
    "# --- Helper function for visualization (assuming it's in a previous cell) ---\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0, 1]) # Source is Magenta\n",
    "    target_temp.paint_uniform_color([1, 1, 0]) # Target is Yellow\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "\n",
    "# --- Step 1: Your Fingerprinting Logic, Optimized with KNN ---\n",
    "def get_robust_fingerprints_knn(pcd, num_neighbors):\n",
    "    \"\"\"\n",
    "    Calculates a robust fingerprint for each point based on the perimeters of\n",
    "    triangles formed with its K-Nearest Neighbors.\n",
    "    \"\"\"\n",
    "    fingerprints = []\n",
    "    points = np.asarray(pcd.points)\n",
    "    kdtree = o3d.geometry.KDTreeFlann(pcd)\n",
    "    \n",
    "    for i in range(len(points)):\n",
    "        ref_point = points[i]\n",
    "        \n",
    "        # Find K nearest neighbors for the current point\n",
    "        [k, idx, _] = kdtree.search_knn_vector_3d(ref_point, num_neighbors + 1)\n",
    "        neighbor_points = points[idx[1:]] # Exclude the point itself\n",
    "        \n",
    "        if len(neighbor_points) < 2:\n",
    "            fingerprints.append([]) # Not enough neighbors to form a triangle\n",
    "            continue\n",
    "        \n",
    "        perimeters = []\n",
    "        # Create all unique pairs of neighbors to form triangles\n",
    "        for n1, n2 in combinations(neighbor_points, 2):\n",
    "            d1 = np.linalg.norm(ref_point - n1)\n",
    "            d2 = np.linalg.norm(ref_point - n2)\n",
    "            d3 = np.linalg.norm(n1 - n2)\n",
    "            perimeters.append(d1 + d2 + d3)\n",
    "        \n",
    "        # The final signature is the sorted list of perimeters\n",
    "        fingerprints.append(sorted(perimeters))\n",
    "        \n",
    "    return fingerprints\n",
    "\n",
    "# --- Step 2: Find Corresponding Point Pairs ---\n",
    "def find_correspondences(fingerprints1, fingerprints2):\n",
    "    \"\"\"Compares the robust fingerprints to find matching pairs.\"\"\"\n",
    "    correspondences = []\n",
    "    unmatched2_indices = list(range(len(fingerprints2)))\n",
    "\n",
    "    for i, sig1 in enumerate(fingerprints1):\n",
    "        if not sig1: continue # Skip if no fingerprint was generated\n",
    "\n",
    "        best_match_idx = -1\n",
    "        min_diff = float('inf')\n",
    "\n",
    "        # Find the best matching fingerprint in the target set\n",
    "        for j in unmatched2_indices:\n",
    "            sig2 = fingerprints2[j]\n",
    "            if not sig2 or len(sig1) != len(sig2): continue\n",
    "            \n",
    "            diff = np.linalg.norm(np.array(sig1) - np.array(sig2))\n",
    "            if diff < min_diff:\n",
    "                min_diff = diff\n",
    "                best_match_idx = j\n",
    "        \n",
    "        # Check if the match is good enough (using a tolerance)\n",
    "        if best_match_idx != -1 and np.allclose(fingerprints1[i], fingerprints2[best_match_idx], atol=0.1):\n",
    "            correspondences.append([i, best_match_idx])\n",
    "            unmatched2_indices.remove(best_match_idx)\n",
    "\n",
    "    return np.array(correspondences)\n",
    "\n",
    "\n",
    "# --- Step 3: Run the Pairing and Estimate Transformation ---\n",
    "\n",
    "# Display initial alignment\n",
    "print(\"1. Displaying initial alignment before custom registration.\")\n",
    "draw_registration_result(source, target, np.identity(4))\n",
    "\n",
    "# --- Parameter to Tune ---\n",
    "# How many neighbors to use for creating the fingerprint.\n",
    "# This is a key parameter to adjust based on your point cloud's density.\n",
    "NUM_NEIGHBORS_FOR_FP = 10 \n",
    "\n",
    "print(f\"2. Calculating fingerprints using {NUM_NEIGHBORS_FOR_FP} nearest neighbors...\")\n",
    "start_time = time.time()\n",
    "fp1 = get_robust_fingerprints_knn(source, NUM_NEIGHBORS_FOR_FP)\n",
    "fp2 = get_robust_fingerprints_knn(target, NUM_NEIGHBORS_FOR_FP)\n",
    "print(f\"   Fingerprinting took {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "print(\"\\n3. Finding correspondences between point clouds...\")\n",
    "correspondences = find_correspondences(fp1, fp2)\n",
    "print(f\"   Found {len(correspondences)} corresponding pairs.\")\n",
    "\n",
    "if len(correspondences) < 3:\n",
    "    print(\"\\n❌ Not enough correspondences found to calculate a transformation. Try adjusting parameters.\")\n",
    "else:\n",
    "    # --- Step 4: Calculate Transformation from Correspondences ---\n",
    "    print(\"\\n4. Estimating transformation from found pairs...\")\n",
    "    \n",
    "    # Use Open3D's standard function to compute the best transformation from pairs\n",
    "    corres_vector = o3d.utility.Vector2iVector(correspondences)\n",
    "    estimation = o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "    transformation_matrix = estimation.compute_transformation(source, target, corres_vector)\n",
    "\n",
    "    # --- Step 5: Final Result ---\n",
    "    print(\"\\n5. Displaying final alignment after custom registration.\")\n",
    "    draw_registration_result(source, target, transformation_matrix)\n",
    "\n",
    "    print(\"\\nRegistration complete.\")\n",
    "    print(\"Final Transformation Matrix:\")\n",
    "    print(transformation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e127c6",
   "metadata": {},
   "source": [
    "# --- 11c. Hybrid Registration with Custom Python RANSAC ---\n",
    "import time\n",
    "import copy\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "# --- Helper functions (no changes here) ---\n",
    "\n",
    "def get_robust_fingerprints_knn(pcd, num_neighbors):\n",
    "    \"\"\"Calculates a fingerprint for each point using its K-Nearest Neighbors.\"\"\"\n",
    "    fingerprints = []\n",
    "    points = np.asarray(pcd.points)\n",
    "    kdtree = o3d.geometry.KDTreeFlann(pcd)\n",
    "    for i in range(len(points)):\n",
    "        ref_point = points[i]\n",
    "        [k, idx, _] = kdtree.search_knn_vector_3d(ref_point, num_neighbors + 1)\n",
    "        neighbor_points = points[idx[1:]]\n",
    "        if len(neighbor_points) < 2:\n",
    "            fingerprints.append(None)\n",
    "            continue\n",
    "        perimeters = [np.linalg.norm(ref_point - n1) + np.linalg.norm(ref_point - n2) + np.linalg.norm(n1 - n2) \n",
    "                      for n1, n2 in combinations(neighbor_points, 2)]\n",
    "        fingerprints.append(sorted(perimeters))\n",
    "    return fingerprints\n",
    "\n",
    "def find_best_matches(fp1, fp2):\n",
    "    \"\"\"Finds the best potential match in fp2 for each fingerprint in fp1.\"\"\"\n",
    "    correspondences = []\n",
    "    for i, sig1 in enumerate(fp1):\n",
    "        if not sig1: continue\n",
    "        best_match_j = -1\n",
    "        min_diff = float('inf')\n",
    "        for j, sig2 in enumerate(fp2):\n",
    "            if not sig2 or len(sig1) != len(sig2): continue\n",
    "            diff = np.linalg.norm(np.array(sig1) - np.array(sig2))\n",
    "            if diff < min_diff:\n",
    "                min_diff = diff\n",
    "                best_match_j = j\n",
    "        if best_match_j != -1:\n",
    "            correspondences.append([i, best_match_j])\n",
    "    return np.asarray(correspondences, dtype=np.int32)\n",
    "\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    \"\"\"Visualizes the registration result.\"\"\"\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0, 1]) # Source is Magenta\n",
    "    target_temp.paint_uniform_color([1, 1, 0]) # Target is Yellow\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "\n",
    "# --- NEW CUSTOM RANSAC FUNCTION ---\n",
    "def run_python_ransac(source_pcd, target_pcd, corres_np, distance_threshold, iterations=1000):\n",
    "    \"\"\"\n",
    "    Performs RANSAC in pure Python to avoid the broken Open3D function.\n",
    "    \"\"\"\n",
    "    best_inlier_count = 0\n",
    "    best_transformation = np.identity(4)\n",
    "    \n",
    "    source_points = np.asarray(source_pcd.points)\n",
    "    target_points = np.asarray(target_pcd.points)\n",
    "    \n",
    "    estimation = o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # 1. Randomly sample 3 correspondence pairs\n",
    "        sample_indices = np.random.choice(len(corres_np), 3, replace=False)\n",
    "        sample_corres = corres_np[sample_indices]\n",
    "        \n",
    "        # 2. Get the 3D points for these pairs\n",
    "        p_source = o3d.geometry.PointCloud()\n",
    "        p_source.points = o3d.utility.Vector3dVector(source_points[sample_corres[:, 0]])\n",
    "        p_target = o3d.geometry.PointCloud()\n",
    "        p_target.points = o3d.utility.Vector3dVector(target_points[sample_corres[:, 1]])\n",
    "        \n",
    "        # 3. Estimate a transformation from this small sample\n",
    "        current_transformation = estimation.compute_transformation(p_source, p_target, o3d.utility.Vector2iVector(np.array([[0,0],[1,1],[2,2]])))\n",
    "        \n",
    "        # 4. Count inliers: check how many of ALL pairs agree with this transformation\n",
    "        source_transformed = copy.deepcopy(source_pcd)\n",
    "        source_transformed.transform(current_transformation)\n",
    "        distances = np.linalg.norm(np.asarray(source_transformed.points)[corres_np[:, 0]] - target_points[corres_np[:, 1]], axis=1)\n",
    "        inlier_count = np.sum(distances < distance_threshold)\n",
    "        \n",
    "        # 5. If this is the best model so far, save it\n",
    "        if inlier_count > best_inlier_count:\n",
    "            best_inlier_count = inlier_count\n",
    "            best_transformation = current_transformation\n",
    "            \n",
    "    # Create a RegistrationResult object to mimic the original function's output\n",
    "    reg_result = o3d.pipelines.registration.RegistrationResult()\n",
    "    reg_result.transformation = best_transformation\n",
    "    return reg_result\n",
    "\n",
    "# --- Main Registration Workflow ---\n",
    "\n",
    "# Step 0: Define Source and Target\n",
    "source = mirrored_processed \n",
    "target = ref_processed\n",
    "\n",
    "# Step 1: Initial Visualization\n",
    "print(\"1. Displaying initial alignment before registration.\")\n",
    "draw_registration_result(source, target, np.identity(4))\n",
    "\n",
    "# Step 2: Generate potential correspondences\n",
    "NUM_NEIGHBORS_FOR_FP = 10 \n",
    "print(f\"\\n2. Calculating fingerprints using {NUM_NEIGHBORS_FOR_FP} nearest neighbors...\")\n",
    "start_time = time.time()\n",
    "fp_source = get_robust_fingerprints_knn(source, NUM_NEIGHBORS_FOR_FP)\n",
    "fp_target = get_robust_fingerprints_knn(target, NUM_NEIGHBORS_FOR_FP)\n",
    "print(f\"   Fingerprinting took {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "print(\"\\n3. Finding potential correspondences...\")\n",
    "potential_corres_np = find_best_matches(fp_source, fp_target)\n",
    "print(f\"   Found {len(potential_corres_np)} potential pairs.\")\n",
    "\n",
    "# Step 3: Global Registration using our new CUSTOM RANSAC function\n",
    "distance_threshold_ransac = voxel_size * 5 \n",
    "print(f\"\\n4. Running CUSTOM Python RANSAC to find the best transformation (threshold: {distance_threshold_ransac:.3f})...\")\n",
    "ransac_result = run_python_ransac(source, target, potential_corres_np, distance_threshold_ransac)\n",
    "\n",
    "print(\"5. Displaying alignment after RANSAC.\")\n",
    "draw_registration_result(source, target, ransac_result.transformation)\n",
    "\n",
    "# Step 4: Local Refinement with ICP\n",
    "print(\"\\n6. Running ICP to refine the alignment...\")\n",
    "source.estimate_normals(o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30))\n",
    "target.estimate_normals(o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30))\n",
    "distance_threshold_icp = voxel_size * 0.4\n",
    "icp_result = o3d.pipelines.registration.registration_icp(\n",
    "    source, target, distance_threshold_icp, ransac_result.transformation,\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "\n",
    "# Step 5: Final Result\n",
    "print(\"\\n7. Displaying final alignment after ICP refinement.\")\n",
    "draw_registration_result(source, target, icp_result.transformation)\n",
    "\n",
    "print(\"\\nHybrid Registration complete.\")\n",
    "print(\"Final Transformation Matrix:\")\n",
    "print(icp_result.transformation)\n",
    "print(f\"Fitness: {icp_result.fitness:.4f}\")\n",
    "print(f\"Inlier RMSE: {icp_result.inlier_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa69c619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting standard registration with feature voxel size: 0.02\n",
      "Displaying alignment after RANSAC...\n",
      "Running ICP to refine the alignment...\n",
      "Displaying final alignment after ICP...\n",
      "\n",
      "Standard Registration complete.\n",
      "Final Transformation Matrix:\n",
      "[[ 0.99230326 -0.03428519  0.11899059  0.15936217]\n",
      " [ 0.04187104  0.99720692 -0.06184804 -0.07267867]\n",
      " [-0.11653777  0.06635427  0.99096723  0.00441674]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Fitness: 0.1667\n",
      "Inlier RMSE: 0.0030\n"
     ]
    }
   ],
   "source": [
    "# --- 11d. Standard Registration (FPFH + RANSAC + ICP) ---\n",
    "import copy\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "# --- Helper Function ---\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0, 1]) # Source is Magenta\n",
    "    target_temp.paint_uniform_color([1, 1, 0]) # Target is Yellow\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "\n",
    "# --- Main Registration Workflow ---\n",
    "\n",
    "# Use the preprocessed clouds with UNIFIED NORMALS from the previous step\n",
    "source = mirrored_processed \n",
    "target = ref_processed\n",
    "\n",
    "# --- PARAMETERS TO TUNE ---\n",
    "# This is the most important parameter. It defines the \"neighborhood size\" for feature calculation. \n",
    "# It should be larger than the 'voxel_size' used for preprocessing.\n",
    "# Try increasing it (e.g., to 0.04, 0.05) or decreasing it to find the best value.\n",
    "voxel_size_feature = 0.02\n",
    "\n",
    "# The RANSAC distance threshold should be proportional to the feature voxel size.\n",
    "distance_threshold_ransac = voxel_size_feature * 0.5\n",
    "\n",
    "\n",
    "print(f\"Starting standard registration with feature voxel size: {voxel_size_feature}\")\n",
    "\n",
    "# --- Global Registration (RANSAC on FPFH Features) ---\n",
    "\n",
    "# Downsample for feature calculation\n",
    "source_down = source.voxel_down_sample(voxel_size_feature)\n",
    "target_down = target.voxel_down_sample(voxel_size_feature)\n",
    "\n",
    "# Estimate normals\n",
    "source_down.estimate_normals(o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size_feature * 2, max_nn=30))\n",
    "target_down.estimate_normals(o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size_feature * 2, max_nn=30))\n",
    "\n",
    "# Compute FPFH features\n",
    "source_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "    source_down, o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size_feature * 5, max_nn=100))\n",
    "target_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "    target_down, o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size_feature * 5, max_nn=100))\n",
    "\n",
    "# Run RANSAC\n",
    "ransac_result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "    source_down, target_down, source_fpfh, target_fpfh, True,\n",
    "    distance_threshold_ransac,\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPoint(False), 4, [\n",
    "        o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),\n",
    "        o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold_ransac)\n",
    "    ], o3d.pipelines.registration.RANSACConvergenceCriteria(4000000, 0.999))\n",
    "\n",
    "print(\"Displaying alignment after RANSAC...\")\n",
    "draw_registration_result(source, target, ransac_result.transformation)\n",
    "\n",
    "# --- Local Refinement (ICP) ---\n",
    "print(\"Running ICP to refine the alignment...\")\n",
    "distance_threshold_icp = voxel_size * 0.4 \n",
    "icp_result = o3d.pipelines.registration.registration_icp(\n",
    "    source, target, distance_threshold_icp, ransac_result.transformation,\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "\n",
    "print(\"Displaying final alignment after ICP...\")\n",
    "draw_registration_result(source, target, icp_result.transformation)\n",
    "\n",
    "print(\"\\nStandard Registration complete.\")\n",
    "print(\"Final Transformation Matrix:\")\n",
    "print(icp_result.transformation)\n",
    "print(f\"Fitness: {icp_result.fitness:.4f}\")\n",
    "print(f\"Inlier RMSE: {icp_result.inlier_rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
